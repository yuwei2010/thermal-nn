{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a63d3676-f91c-4043-bad8-3bc7ce147043",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install numpy pandas torch tqdm optuna\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Parameter as TorchParam\n",
    "from torch import Tensor\n",
    "from typing import List, Tuple\n",
    "from joblib import parallel_backend\n",
    "\n",
    "\n",
    "import optuna\n",
    "from torch.jit import ScriptModule, script_method\n",
    "\n",
    "MeasurementData_Merged = pd.read_csv(\"MeasurementData_Merged_EFAD.csv\")\n",
    "# MeasurementData_Merged = MeasurementData_Merged.rename(columns={'Rotor Temperature TelemetriesExternTemp.rp_rbe_Cif_10ms_PIExternTemp1.rbe_Cif': 'ExternTemp.rp_rbe_Cif_10ms_PIExternTemp1.rbe_Cif', 'Stator Temperature TelemetriesI_EM_tTMotWinDeLay2Cl03': 'I_EM_tTMotWinDeLay2Cl03'})\n",
    "# MeasurementData_Merged.to_pickle('MeasurementData_Merged.pkl')\n",
    "MeasurementData_Merged['pm'] = MeasurementData_Merged[['ExternTemp.rp_rbe_Cif_10ms_PIExternTemp1.rbe_Cif', 'ExternTemp.rp_rbe_Cif_10ms_PIExternTemp10.rbe_Cif', 'ExternTemp.rp_rbe_Cif_10ms_PIExternTemp5.rbe_Cif','ExternTemp.rp_rbe_Cif_10ms_PIExternTemp7.rbe_Cif']].mean(axis=1)\n",
    "MeasurementData_Merged['stator_winding'] = MeasurementData_Merged[['I_EM_tTMotWinDeLay2Cl03', 'I_EM_tTMotWinDeLay2Cl12','I_EM_tTMotWinDeLay5Cl03','I_EM_tTMotWinDeLay5Cl06','I_EM_tTMotWinDeLay5Cl12',\n",
    "                                                                   'I_EM_tTMotWinNdeLay2Cl03','I_EM_tTMotWinNdeLay2Cl12','I_EM_tTMotWinNdeLay5Cl03']].mean(axis=1)\n",
    "MeasurementData_Merged['Us'] = np.sqrt(MeasurementData_Merged['uDaFundaFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm']**2 + MeasurementData_Merged['uQaFundaFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm']**2)\n",
    "MeasurementData_Merged['Is'] = np.sqrt(MeasurementData_Merged['iDaFild10ms.Rec_2ms_Fild.rp_rbe_CddIPha_2ms_Fild.rbe_MctAsm']**2 + MeasurementData_Merged['iQaFild10ms.Rec_2ms_Fild.rp_rbe_CddIPha_2ms_Fild.rbe_MctAsm']**2)\n",
    "\n",
    "# input_cols = ['nEmFild100ms.Rec_10ms_Fild.pp_rbe_CddAgEm_10ms_Fild.rbe_CddRslvr',\n",
    "#                 'tqEmFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm',\n",
    "#                 'iDaFild10ms.Rec_2ms_Fild.rp_rbe_CddIPha_2ms_Fild.rbe_MctAsm',\n",
    "#                 'iQaFild10ms.Rec_2ms_Fild.rp_rbe_CddIPha_2ms_Fild.rbe_MctAsm',\n",
    "#                 'uDaFundaFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm',\n",
    "#                 'uQaFundaFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm',\n",
    "#                 'R_CW_tCooltIvtrOut',\n",
    "#                 # 'R_EM_tTMotRshaftOilIn',\n",
    "#                 'tEmSnsrFild10ms.Rec_2ms_Fild.rp_rbe_CddTEm_2ms_Fild.rbe_TMdlEm',\n",
    "#                 'Us',\n",
    "#                 'Is']\n",
    "\n",
    "input_cols = ['nEmFild100ms.Rec_10ms_Fild.pp_rbe_CddAgEm_10ms_Fild.rbe_CddRslvr',\n",
    "                'tqEmFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm',\n",
    "                'iDaFild10ms.Rec_2ms_Fild.rp_rbe_CddIPha_2ms_Fild.rbe_MctAsm',\n",
    "                'iQaFild10ms.Rec_2ms_Fild.rp_rbe_CddIPha_2ms_Fild.rbe_MctAsm',\n",
    "                'uDaFundaFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm',\n",
    "                'uQaFundaFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm',\n",
    "                'R_CW_tCooltIvtrOut',\n",
    "                # 'R_EM_tTMotRshaftOilIn',\n",
    "                'tEmSnsrFild10ms.Rec_2ms_Fild.rp_rbe_CddTEm_2ms_Fild.rbe_TMdlEm']\n",
    "\n",
    "# path_to_csv = Path().cwd() / \"data\" / \"input\" / \"measures_v2.csv\"\n",
    "data = MeasurementData_Merged.copy()\n",
    "data = data[data['pm'] <= 200]\n",
    "# data = data[data['stator_winding'] <= 150]\n",
    "# target_cols = ['pm', 'stator_winding']\n",
    "# target_cols = ['pm']\n",
    "# target_cols = ['stator_winding']\n",
    "target_cols = ['pm','stator_winding','R_EM_tTMotRshaftOilIn']\n",
    "temperature_cols = target_cols + ['tEmSnsrFild10ms.Rec_2ms_Fild.rp_rbe_CddTEm_2ms_Fild.rbe_TMdlEm','R_CW_tCooltIvtrOut']\n",
    "test_profiles = [36, 37]\n",
    "test_blacklist_profiles = [25]\n",
    "train_profiles = [p for p in data.profile_id.unique() if p not in test_profiles and p not in test_blacklist_profiles]\n",
    "profile_sizes = data.groupby(\"profile_id\").agg(\"size\")\n",
    "\n",
    "# normalize\n",
    "non_temperature_cols = [c for c in data if c in input_cols and c not in temperature_cols]\n",
    "data.loc[:, temperature_cols] /= 200  # deg C\n",
    "data.loc[:, non_temperature_cols] /= data.loc[:, non_temperature_cols].abs().max(axis=0)\n",
    "\n",
    "# # extra feats (FE)\n",
    "# if {\"i_d\", \"i_q\", \"u_d\", \"u_q\"}.issubset(set(data.columns.tolist())):\n",
    "#     extra_feats = {\n",
    "#         \"i_s\": lambda x: np.sqrt((x[\"i_d\"] ** 2 + x[\"i_q\"] ** 2)),\n",
    "#         \"u_s\": lambda x: np.sqrt((x[\"u_d\"] ** 2 + x[\"u_q\"] ** 2)),\n",
    "#     }\n",
    "# data = data.assign(**extra_feats)\n",
    "# input_cols = [c for c in data.columns if c not in target_cols]\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# overwrite. We recommend CPU over GPU here, as that runs faster with pytorch on this data set\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Rearrange features\n",
    "# input_cols = [c for c in data.columns if c not in target_cols + [\"profile_id\"]]\n",
    "data = data.loc[:, input_cols + [\"profile_id\"] + target_cols].dropna()\n",
    "\n",
    "def generate_tensor(profiles_list):\n",
    "    \"\"\"Returns profiles of the data set in a coherent 3D tensor with\n",
    "    time-major shape (T, B, F) where\n",
    "    T : Maximum profile length\n",
    "    B : Batch size = Amount of profiles\n",
    "    F : Amount of input features.\n",
    "\n",
    "    Also returns a likewise-shaped sample_weights tensor, which zeros out post-padded zeros for use\n",
    "    in the cost function (i.e., it acts as masking tensor)\"\"\"\n",
    "    tensor = np.full(\n",
    "        (profile_sizes[profiles_list].max(), len(profiles_list), data.shape[1] - 1),\n",
    "        np.nan,\n",
    "    )\n",
    "    for i, (pid, df) in enumerate(\n",
    "        data.loc[data.profile_id.isin(profiles_list), :].groupby(\"profile_id\")\n",
    "    ):\n",
    "        assert pid in profiles_list, f\"PID is not in {profiles_list}!\"\n",
    "        tensor[: len(df), i, :] = df.drop(columns=\"profile_id\").to_numpy()\n",
    "    sample_weights = 1 - np.isnan(tensor[:, :, 0])\n",
    "    tensor = np.nan_to_num(tensor).astype(np.float32)\n",
    "    tensor = torch.from_numpy(tensor).to(device)\n",
    "    sample_weights = torch.from_numpy(sample_weights).to(device)\n",
    "    return tensor, sample_weights\n",
    "\n",
    "\n",
    "train_tensor, train_sample_weights = generate_tensor(train_profiles)\n",
    "test_tensor, test_sample_weights = generate_tensor(test_profiles)\n",
    "\n",
    "# Hyper parameters optimization \n",
    "# 自定义正弦激活层\n",
    "class SinusLayer(nn.Module):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sin(x)\n",
    "    \n",
    "def smooth_abs(x: torch.Tensor, epsilon: float = 1e-6) -> torch.Tensor:\n",
    "    return torch.sqrt(x**2 + epsilon)\n",
    "\n",
    "# 激活函数映射\n",
    "def get_activation(activation_name: str) -> nn.Module:\n",
    "    activation_dict = {\n",
    "        \"Sigmoid\": nn.Sigmoid(),\n",
    "        \"tanh\": nn.Tanh(),\n",
    "        \"linear\": nn.Identity(),\n",
    "        \"ReLU\": nn.ReLU(),\n",
    "        \"biased Elu\": nn.ELU(alpha=1.0),\n",
    "        \"sinus\": SinusLayer()\n",
    "    }\n",
    "    return activation_dict[activation_name]\n",
    "\n",
    "# TNNCell定义（支持动态结构和激活函数）\n",
    "class TNNCell(nn.Module):\n",
    "    def __init__(self, cond_net_layers, cond_net_units, cond_activations, \n",
    "                 ploss_net_layers, ploss_net_units, ploss_activations):\n",
    "        super().__init__()\n",
    "        self.sample_time = 0.5\n",
    "        self.output_size = len(target_cols)\n",
    "        self.caps = nn.Parameter(torch.Tensor(self.output_size))\n",
    "        nn.init.normal_(self.caps, mean=-9.2, std=0.5)\n",
    "        \n",
    "        n_temps = len(temperature_cols)\n",
    "        n_conds = int(0.5 * n_temps * (n_temps - 1))\n",
    "        \n",
    "        # 动态构建conductance_net\n",
    "        cond_layers = []\n",
    "        input_dim = len(input_cols) + self.output_size\n",
    "        for i, units in enumerate(cond_net_units):\n",
    "            cond_layers.append(nn.Linear(input_dim, units))\n",
    "            # 使用当前层的激活函数\n",
    "            cond_layers.append(get_activation(cond_activations[i]))\n",
    "            input_dim = units\n",
    "        cond_layers.append(nn.Linear(input_dim, n_conds))\n",
    "        cond_layers.append(nn.Sigmoid())\n",
    "        self.conductance_net = nn.Sequential(*cond_layers)\n",
    "        \n",
    "        # 动态构建ploss_net\n",
    "        ploss_layers = []\n",
    "        input_dim = len(input_cols) + self.output_size\n",
    "        for i, units in enumerate(ploss_net_units):\n",
    "            ploss_layers.append(nn.Linear(input_dim, units))\n",
    "            if i < len(ploss_net_units) - 1:\n",
    "                ploss_layers.append(get_activation(ploss_activations[i]))\n",
    "            input_dim = units\n",
    "        ploss_layers.append(nn.Linear(input_dim, self.output_size))\n",
    "        self.ploss = nn.Sequential(*ploss_layers)\n",
    "        \n",
    "        # 其余初始化代码\n",
    "        self.adj_mat = np.zeros((n_temps, n_temps), dtype=int)\n",
    "        triu_idx = np.triu_indices(n_temps, 1)\n",
    "        adj_idx_arr = np.ones_like(self.adj_mat)\n",
    "        adj_idx_arr = adj_idx_arr[triu_idx].ravel()\n",
    "        self.adj_mat[triu_idx] = np.cumsum(adj_idx_arr) - 1\n",
    "        self.adj_mat += self.adj_mat.T\n",
    "        self.adj_mat = torch.from_numpy(self.adj_mat[:self.output_size, :]).type(torch.int64)\n",
    "        self.temp_idcs = [i for i, x in enumerate(input_cols) if x in temperature_cols]\n",
    "        self.nontemp_idcs = [i for i, x in enumerate(input_cols) if x not in temperature_cols + [\"profile_id\"]]\n",
    "\n",
    "    def forward(self, inp: Tensor, hidden: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        prev_out = hidden\n",
    "        temps = torch.cat([prev_out, inp[:, self.temp_idcs]], dim=1)\n",
    "        sub_nn_inp = torch.cat([inp, prev_out], dim=1)\n",
    "        # conducts = torch.abs(self.conductance_net(sub_nn_inp))\n",
    "        conducts = self.conductance_net(sub_nn_inp)\n",
    "        # power_loss = torch.abs(self.ploss(sub_nn_inp))\n",
    "        power_loss = smooth_abs(self.ploss(sub_nn_inp))\n",
    "        temp_diffs = torch.sum(\n",
    "            (temps.unsqueeze(1) - prev_out.unsqueeze(-1)) * conducts[:, self.adj_mat],\n",
    "            dim=-1,\n",
    "        )\n",
    "        out = prev_out + self.sample_time * torch.exp(self.caps) * (temp_diffs + power_loss)\n",
    "        return prev_out, torch.clip(out, -1, 5)\n",
    "\n",
    "# DiffEqLayer定义（支持TorchScript）\n",
    "class DiffEqLayer(ScriptModule):\n",
    "    def __init__(self, cell_module):\n",
    "        super().__init__()\n",
    "        self.cell = cell_module\n",
    "        \n",
    "    @script_method\n",
    "    def forward(self, input: Tensor, state: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        inputs = input.unbind(0)\n",
    "        outputs = []\n",
    "        for i in range(len(inputs)):\n",
    "            out, state = self.cell(inputs[i], state)\n",
    "            outputs.append(out)\n",
    "        return torch.stack(outputs), state\n",
    "\n",
    "# Optuna目标函数\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    # 1. 超参数采样\n",
    "    cond_net_layers = trial.suggest_int('cond_net_layers', 1, 3)\n",
    "    cond_net_units = [trial.suggest_int(f'cond_net_units_{i}', 2, 30) \n",
    "                     for i in range(cond_net_layers)]\n",
    "    \n",
    "    ploss_net_layers = trial.suggest_int('ploss_net_layers', 1, 3)\n",
    "    ploss_net_units = [trial.suggest_int(f'ploss_net_units_{i}', 2, 30) \n",
    "                      for i in range(ploss_net_layers)]\n",
    "    \n",
    "    # 为cond_net每层单独采样激活函数\n",
    "    cond_activations = [\n",
    "        trial.suggest_categorical(f'cond_activation_{i}', \n",
    "        [\"Sigmoid\", \"tanh\", \"linear\", \"ReLU\", \"biased Elu\", \"sinus\"])\n",
    "        for i in range(cond_net_layers)\n",
    "    ]\n",
    "    # 为ploss_net每层单独采样激活函数\n",
    "    ploss_activations = [\n",
    "        trial.suggest_categorical(f'ploss_activation_{i}', \n",
    "        [\"Sigmoid\", \"tanh\", \"linear\", \"ReLU\", \"biased Elu\", \"sinus\"])\n",
    "        for i in range(ploss_net_layers)\n",
    "    ]\n",
    "    \n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'NAdam'])\n",
    "    tbptt_size = trial.suggest_int('tbptt_size', 50, 512)\n",
    "\n",
    "    # 构建模型（传递激活函数列表）\n",
    "    tnn_cell = TNNCell(\n",
    "        cond_net_layers, cond_net_units, cond_activations,  # 传递列表\n",
    "        ploss_net_layers, ploss_net_units, ploss_activations  # 传递列表\n",
    "    ).to(device)\n",
    "    \n",
    "    model = DiffEqLayer(tnn_cell).to(device)\n",
    "    \n",
    "    # 3. 选择优化器\n",
    "    if optimizer_name == 'Adam':\n",
    "        opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        opt = optim.NAdam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # 4. 训练与评估（使用训练集本身）\n",
    "    n_epochs = 100\n",
    "    loss_func = nn.MSELoss(reduction=\"none\")\n",
    "    best_train_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        hidden = train_tensor[0, :, -len(target_cols):]\n",
    "        n_batches = int(np.ceil(train_tensor.shape[0] / tbptt_size))\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        model.train()\n",
    "        for i in range(n_batches):\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            output, hidden = model(\n",
    "                train_tensor[i*tbptt_size : (i+1)*tbptt_size, :, :len(input_cols)],\n",
    "                hidden.detach().requires_grad_(True)\n",
    "            )\n",
    "            \n",
    "            # 计算训练损失\n",
    "            loss = loss_func(\n",
    "                output,\n",
    "                train_tensor[i*tbptt_size : (i+1)*tbptt_size, :, -len(target_cols):]\n",
    "            )\n",
    "            \n",
    "            # 加权损失计算\n",
    "            loss = (loss * train_sample_weights[i*tbptt_size:(i+1)*tbptt_size, :, None]).sum() \n",
    "            loss /= train_sample_weights[i*tbptt_size:(i+1)*tbptt_size, :].sum() + 1e-8  # 防止除零\n",
    "            \n",
    "            # 添加梯度检查（提前拦截问题）[9](@ref)\n",
    "            if not loss.requires_grad:\n",
    "                print(f\"梯度断裂! Trial: {trial.number}, Batch: {i}\")\n",
    "                raise optuna.TrialPruned()\n",
    "            # 反向传播\n",
    "            try:\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                epoch_loss += loss.item()\n",
    "            except RuntimeError as e:\n",
    "                if \"does not have a grad_fn\" in str(e):\n",
    "                    raise optuna.TrialPruned()\n",
    "\n",
    "        # 计算平均训练损失\n",
    "        avg_epoch_loss = epoch_loss / n_batches\n",
    "        \n",
    "        # 报告训练损失给Optuna\n",
    "        trial.report(avg_epoch_loss, epoch)\n",
    "        \n",
    "        # 剪枝逻辑（基于训练损失）\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        # 记录最佳训练损失\n",
    "        if avg_epoch_loss < best_train_loss:\n",
    "            best_train_loss = avg_epoch_loss\n",
    "    \n",
    "    return best_train_loss\n",
    "\n",
    "# # 创建Optuna研究\n",
    "# study = optuna.create_study(\n",
    "#     direction='minimize',\n",
    "#     sampler=optuna.samplers.TPESampler(),\n",
    "#     pruner=optuna.pruners.MedianPruner(\n",
    "#         n_startup_trials=5,\n",
    "#         n_warmup_steps=10\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# 配置共享存储（SQLite或MySQL）\n",
    "storage_name = \"sqlite:///optuna.db\"\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(),\n",
    "    storage=storage_name,\n",
    "    study_name=\"tnn_optimization\",\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "# 并行执行（n_jobs为并行进程数）\n",
    "with parallel_backend(\"threading\", n_jobs=8):  # 或使用多进程\"multiprocessing\"\n",
    "    study.optimize(objective, n_trials=1000, show_progress_bar=True)\n",
    "\n",
    "\n",
    "# # 启动优化\n",
    "# study.optimize(objective, n_trials=200, show_progress_bar=True)\n",
    "\n",
    "# 输出结果\n",
    "print(\"最佳训练损失:\", study.best_value)\n",
    "print(\"最佳参数组合:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# 使用最佳参数训练最终模型\n",
    "best_params = study.best_params\n",
    "\n",
    "# 收集cond_net每层的激活函数\n",
    "cond_activations = [\n",
    "    best_params[f'cond_activation_{i}'] \n",
    "    for i in range(best_params['cond_net_layers'])\n",
    "]\n",
    "\n",
    "# 收集ploss_net每层的激活函数\n",
    "ploss_activations = [\n",
    "    best_params[f'ploss_activation_{i}'] \n",
    "    for i in range(best_params['ploss_net_layers'])\n",
    "]\n",
    "\n",
    "final_tnn_cell = TNNCell(\n",
    "    cond_net_layers=best_params['cond_net_layers'],\n",
    "    cond_net_units=[best_params[f'cond_net_units_{i}'] for i in range(best_params['cond_net_layers'])],\n",
    "    cond_activations=cond_activations,  # 传递激活函数列表\n",
    "    ploss_net_layers=best_params['ploss_net_layers'],\n",
    "    ploss_net_units=[best_params[f'ploss_net_units_{i}'] for i in range(best_params['ploss_net_layers'])],\n",
    "    ploss_activations=ploss_activations  # 传递激活函数列表\n",
    ").to(device)\n",
    "\n",
    "final_model = torch.jit.script(DiffEqLayer(final_tnn_cell)).to(device)\n",
    "# 从Optuna最佳参数中提取优化器名称\n",
    "optimizer_name = best_params['optimizer']\n",
    "# 动态创建优化器\n",
    "if optimizer_name == 'Adam':\n",
    "    opt = optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
    "else:  # 对应NAdam\n",
    "    opt = optim.NAdam(final_model.parameters(), lr=best_params['lr'])\n",
    "# 最终训练\n",
    "n_epochs = 500\n",
    "tbptt_size = best_params['tbptt_size']\n",
    "loss_func = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "with tqdm(desc=\"Final Training\", total=n_epochs) as pbar:\n",
    "    for epoch in range(n_epochs):\n",
    "        hidden = train_tensor[0, :, -len(target_cols):]\n",
    "        n_batches = int(np.ceil(train_tensor.shape[0] / tbptt_size))\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            opt.zero_grad()\n",
    "            output, hidden = final_model(\n",
    "                train_tensor[i*tbptt_size : (i+1)*tbptt_size, :, :len(input_cols)],\n",
    "                hidden.detach()\n",
    "            )\n",
    "            \n",
    "            loss = loss_func(\n",
    "                output,\n",
    "                train_tensor[i*tbptt_size : (i+1)*tbptt_size, :, -len(target_cols):]\n",
    "            )\n",
    "            \n",
    "            loss = (loss * train_sample_weights[i*tbptt_size:(i+1)*tbptt_size, :, None]).sum()\n",
    "            loss /= train_sample_weights[i*tbptt_size:(i+1)*tbptt_size, :].sum()\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        # 学习率衰减\n",
    "        if epoch == int(n_epochs*0.75):\n",
    "            for group in opt.param_groups:\n",
    "                group[\"lr\"] *= 0.5\n",
    "        \n",
    "        pbar.update()\n",
    "        pbar.set_postfix_str(f\"loss: {loss.item():.2e}\")\n",
    "\n",
    "# model saving and loading\n",
    "mdl_path = Path.cwd() / 'data' / 'models'\n",
    "mdl_path.mkdir(exist_ok=True, parents=True)\n",
    "# mdl_file_path = mdl_path / 'tnn_jit_torch.pt'\n",
    "mdl_file_path = mdl_path / 'tnn_jit_torch_STM_RTM_Oil_Joint_EFAD.pt'\n",
    "# mdl_file_path = mdl_path / 'tnn_jit_torch_RTM_EFAD.pt'\n",
    "final_model.save(mdl_file_path)  # save\n",
    "final_model = torch.jit.load(mdl_file_path)  # load\n",
    "final_model.eval()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Hyper_Parameters_Optimization_TNN",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
