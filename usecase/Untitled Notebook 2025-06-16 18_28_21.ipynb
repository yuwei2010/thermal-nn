{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04a3a5aa-8782-4216-8d43-9f16079b7e8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %pip install numpy pandas matplotlib torch scipy tqdm plotly optuna\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Parameter as TorchParam\n",
    "from torch import Tensor\n",
    "from typing import List, Tuple\n",
    "\n",
    "from scipy.stats import norm\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "import optuna\n",
    "from torch.jit import ScriptModule, script_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71829918-5f0e-45f4-94cf-e1bcd3dab8c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "366632d2-4652-49da-9e52-3c0ff0c014b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MeasurementData_Merged = pd.read_csv(\"MeasurementData_Merged_EFAD.csv\")\n",
    "# MeasurementData_Merged = MeasurementData_Merged.rename(columns={'Rotor Temperature TelemetriesExternTemp.rp_rbe_Cif_10ms_PIExternTemp1.rbe_Cif': 'ExternTemp.rp_rbe_Cif_10ms_PIExternTemp1.rbe_Cif', 'Stator Temperature TelemetriesI_EM_tTMotWinDeLay2Cl03': 'I_EM_tTMotWinDeLay2Cl03'})\n",
    "# MeasurementData_Merged.to_pickle('MeasurementData_Merged.pkl')\n",
    "MeasurementData_Merged['pm'] = MeasurementData_Merged[['ExternTemp.rp_rbe_Cif_10ms_PIExternTemp1.rbe_Cif', 'ExternTemp.rp_rbe_Cif_10ms_PIExternTemp10.rbe_Cif', 'ExternTemp.rp_rbe_Cif_10ms_PIExternTemp5.rbe_Cif','ExternTemp.rp_rbe_Cif_10ms_PIExternTemp7.rbe_Cif']].mean(axis=1)\n",
    "MeasurementData_Merged['stator_winding'] = MeasurementData_Merged[['I_EM_tTMotWinDeLay2Cl03', 'I_EM_tTMotWinDeLay2Cl12','I_EM_tTMotWinDeLay5Cl03','I_EM_tTMotWinDeLay5Cl06','I_EM_tTMotWinDeLay5Cl12',\n",
    "                                                                   'I_EM_tTMotWinNdeLay2Cl03','I_EM_tTMotWinNdeLay2Cl12','I_EM_tTMotWinNdeLay5Cl03']].mean(axis=1)\n",
    "MeasurementData_Merged['Us'] = np.sqrt(MeasurementData_Merged['uDaFundaFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm']**2 + MeasurementData_Merged['uQaFundaFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm']**2)\n",
    "MeasurementData_Merged['Is'] = np.sqrt(MeasurementData_Merged['iDaFild10ms.Rec_2ms_Fild.rp_rbe_CddIPha_2ms_Fild.rbe_MctAsm']**2 + MeasurementData_Merged['iQaFild10ms.Rec_2ms_Fild.rp_rbe_CddIPha_2ms_Fild.rbe_MctAsm']**2)\n",
    "\n",
    "# input_cols = ['nEmFild100ms.Rec_10ms_Fild.pp_rbe_CddAgEm_10ms_Fild.rbe_CddRslvr',\n",
    "#                 'tqEmFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm',\n",
    "#                 'iDaFild10ms.Rec_2ms_Fild.rp_rbe_CddIPha_2ms_Fild.rbe_MctAsm',\n",
    "#                 'iQaFild10ms.Rec_2ms_Fild.rp_rbe_CddIPha_2ms_Fild.rbe_MctAsm',\n",
    "#                 'uDaFundaFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm',\n",
    "#                 'uQaFundaFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm',\n",
    "#                 'R_CW_tCooltIvtrOut',\n",
    "#                 # 'R_EM_tTMotRshaftOilIn',\n",
    "#                 'tEmSnsrFild10ms.Rec_2ms_Fild.rp_rbe_CddTEm_2ms_Fild.rbe_TMdlEm',\n",
    "#                 'Us',\n",
    "#                 'Is']\n",
    "\n",
    "input_cols = ['nEmFild100ms.Rec_10ms_Fild.pp_rbe_CddAgEm_10ms_Fild.rbe_CddRslvr',\n",
    "                'tqEmFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm',\n",
    "                'iDaFild10ms.Rec_2ms_Fild.rp_rbe_CddIPha_2ms_Fild.rbe_MctAsm',\n",
    "                'iQaFild10ms.Rec_2ms_Fild.rp_rbe_CddIPha_2ms_Fild.rbe_MctAsm',\n",
    "                'uDaFundaFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm',\n",
    "                'uQaFundaFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm',\n",
    "                'R_CW_tCooltIvtrOut',\n",
    "                # 'R_EM_tTMotRshaftOilIn',\n",
    "                'tEmSnsrFild10ms.Rec_2ms_Fild.rp_rbe_CddTEm_2ms_Fild.rbe_TMdlEm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc72fa2-08bd-4207-a264-001192b99577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# path_to_csv = Path().cwd() / \"data\" / \"input\" / \"measures_v2.csv\"\n",
    "data = MeasurementData_Merged.copy()\n",
    "data = data[data['pm'] <= 200]\n",
    "# data = data[data['stator_winding'] <= 150]\n",
    "# target_cols = ['pm', 'stator_winding']\n",
    "# target_cols = ['pm']\n",
    "# target_cols = ['stator_winding']\n",
    "target_cols = ['pm','stator_winding','R_EM_tTMotRshaftOilIn']\n",
    "temperature_cols = target_cols + ['tEmSnsrFild10ms.Rec_2ms_Fild.rp_rbe_CddTEm_2ms_Fild.rbe_TMdlEm','R_CW_tCooltIvtrOut']\n",
    "test_profiles = [36, 37]\n",
    "train_profiles = [p for p in data.profile_id.unique() if p not in test_profiles]\n",
    "profile_sizes = data.groupby(\"profile_id\").agg(\"size\")\n",
    "\n",
    "# normalize\n",
    "non_temperature_cols = [c for c in data if c in input_cols and c not in temperature_cols]\n",
    "data.loc[:, temperature_cols] /= 200  # deg C\n",
    "data.loc[:, non_temperature_cols] /= data.loc[:, non_temperature_cols].abs().max(axis=0)\n",
    "\n",
    "# # extra feats (FE)\n",
    "# if {\"i_d\", \"i_q\", \"u_d\", \"u_q\"}.issubset(set(data.columns.tolist())):\n",
    "#     extra_feats = {\n",
    "#         \"i_s\": lambda x: np.sqrt((x[\"i_d\"] ** 2 + x[\"i_q\"] ** 2)),\n",
    "#         \"u_s\": lambda x: np.sqrt((x[\"u_d\"] ** 2 + x[\"u_q\"] ** 2)),\n",
    "#     }\n",
    "# data = data.assign(**extra_feats)\n",
    "# input_cols = [c for c in data.columns if c not in target_cols]\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# overwrite. We recommend CPU over GPU here, as that runs faster with pytorch on this data set\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d49185b5-07e4-4f03-880a-4ed238fbbc4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rearrange features\n",
    "# input_cols = [c for c in data.columns if c not in target_cols + [\"profile_id\"]]\n",
    "data = data.loc[:, input_cols + [\"profile_id\"] + target_cols].dropna()\n",
    "\n",
    "def generate_tensor(profiles_list):\n",
    "    \"\"\"Returns profiles of the data set in a coherent 3D tensor with\n",
    "    time-major shape (T, B, F) where\n",
    "    T : Maximum profile length\n",
    "    B : Batch size = Amount of profiles\n",
    "    F : Amount of input features.\n",
    "\n",
    "    Also returns a likewise-shaped sample_weights tensor, which zeros out post-padded zeros for use\n",
    "    in the cost function (i.e., it acts as masking tensor)\"\"\"\n",
    "    tensor = np.full(\n",
    "        (profile_sizes[profiles_list].max(), len(profiles_list), data.shape[1] - 1),\n",
    "        np.nan,\n",
    "    )\n",
    "    for i, (pid, df) in enumerate(\n",
    "        data.loc[data.profile_id.isin(profiles_list), :].groupby(\"profile_id\")\n",
    "    ):\n",
    "        assert pid in profiles_list, f\"PID is not in {profiles_list}!\"\n",
    "        tensor[: len(df), i, :] = df.drop(columns=\"profile_id\").to_numpy()\n",
    "    sample_weights = 1 - np.isnan(tensor[:, :, 0])\n",
    "    tensor = np.nan_to_num(tensor).astype(np.float32)\n",
    "    tensor = torch.from_numpy(tensor).to(device)\n",
    "    sample_weights = torch.from_numpy(sample_weights).to(device)\n",
    "    return tensor, sample_weights\n",
    "\n",
    "\n",
    "train_tensor, train_sample_weights = generate_tensor(train_profiles)\n",
    "test_tensor, test_sample_weights = generate_tensor(test_profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "506f5870-610a-4c5f-95b9-9f99d0c6fe15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e09ec643-e888-49fc-8466-fac565a66a52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DiffEqLayer(nn.Module):\n",
    "    \"\"\"This class is a container for the computation logic in each step.\n",
    "    This layer could be used for any 'cell', also RNNs, LSTMs or GRUs.\"\"\"\n",
    "\n",
    "    def __init__(self, cell, *cell_args):\n",
    "        super().__init__()\n",
    "        self.cell = cell(*cell_args)\n",
    "\n",
    "    def forward(self, input: Tensor, state: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        inputs = input.unbind(0)\n",
    "        outputs = torch.jit.annotate(List[Tensor], [])\n",
    "        for i in range(len(inputs)):\n",
    "            out, state = self.cell(inputs[i], state)\n",
    "            outputs += [out]\n",
    "        return torch.stack(outputs), state\n",
    "\n",
    "\n",
    "class TNNCell(nn.Module):\n",
    "    \"\"\"The main TNN logic. Here, the sub-NNs are initialized as well as the constant learnable\n",
    "    thermal capacitances. The forward function houses the LPTN ODE discretized with the explicit Euler method\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sample_time = 0.5  # in s\n",
    "        self.output_size = len(target_cols)\n",
    "        self.caps = TorchParam(torch.Tensor(self.output_size).to(device))\n",
    "        nn.init.normal_(\n",
    "            self.caps, mean=-9.2, std=0.5\n",
    "        )  # hand-picked init mean, might be application-dependent\n",
    "        n_temps = len(temperature_cols)  # number of temperatures (targets and input)\n",
    "        n_conds = int(0.5 * n_temps * (n_temps - 1))  # number of thermal conductances\n",
    "        # conductance net sub-NN\n",
    "        self.conductance_net = nn.Sequential(\n",
    "            nn.Linear(len(input_cols) + self.output_size, n_conds), \n",
    "            nn.ReLU(),         # Activation function added by Yuping\n",
    "            nn.Linear(n_conds, n_conds), # Additional Layer added by Yuping\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # populate adjacency matrix. It is used for indexing the conductance sub-NN output\n",
    "        self.adj_mat = np.zeros((n_temps, n_temps), dtype=int)\n",
    "        adj_idx_arr = np.ones_like(self.adj_mat)\n",
    "        triu_idx = np.triu_indices(n_temps, 1)\n",
    "        adj_idx_arr = adj_idx_arr[triu_idx].ravel()\n",
    "        self.adj_mat[triu_idx] = np.cumsum(adj_idx_arr) - 1\n",
    "        self.adj_mat += self.adj_mat.T\n",
    "        self.adj_mat = torch.from_numpy(self.adj_mat[: self.output_size, :]).type(\n",
    "            torch.int64\n",
    "        )  # crop\n",
    "        self.n_temps = n_temps\n",
    "\n",
    "        # power loss sub-NN\n",
    "        self.ploss = nn.Sequential(\n",
    "            nn.Linear(len(input_cols) + self.output_size, 16),\n",
    "            nn.ReLU(),         # Activation function added by Yuping\n",
    "            nn.Linear(16, 16), # Additional Layer added by Yuping\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, self.output_size),\n",
    "        )\n",
    "\n",
    "        self.temp_idcs = [i for i, x in enumerate(input_cols) if x in temperature_cols]\n",
    "        self.nontemp_idcs = [\n",
    "            i\n",
    "            for i, x in enumerate(input_cols)\n",
    "            if x not in temperature_cols + [\"profile_id\"]\n",
    "        ]\n",
    "\n",
    "    def forward(self, inp: Tensor, hidden: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        prev_out = hidden\n",
    "        temps = torch.cat([prev_out, inp[:, self.temp_idcs]], dim=1)\n",
    "        sub_nn_inp = torch.cat([inp, prev_out], dim=1)\n",
    "        conducts = torch.abs(self.conductance_net(sub_nn_inp))\n",
    "        power_loss = torch.abs(self.ploss(sub_nn_inp))\n",
    "        temp_diffs = torch.sum(\n",
    "            (temps.unsqueeze(1) - prev_out.unsqueeze(-1)) * conducts[:, self.adj_mat],\n",
    "            dim=-1,\n",
    "        )\n",
    "        out = prev_out + self.sample_time * torch.exp(self.caps) * (\n",
    "            temp_diffs + power_loss\n",
    "        )\n",
    "        return prev_out, torch.clip(out, -1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd78b870-1eac-4ca3-9a12-4a9ebed8ca50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb495609-5acc-47d1-a332-576753c5b09f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraining:   0%|          | 0/500 [00:00<?, ?it/s]\rTraining:   0%|          | 1/500 [00:03<29:37,  3.56s/it]\rTraining:   0%|          | 1/500 [00:03<29:37,  3.56s/it, loss: 5.14e-02]\rTraining:   0%|          | 2/500 [00:06<26:49,  3.23s/it, loss: 5.14e-02]\rTraining:   0%|          | 2/500 [00:06<26:49,  3.23s/it, loss: 2.30e-02]\rTraining:   1%|          | 3/500 [00:09<26:14,  3.17s/it, loss: 2.30e-02]\rTraining:   1%|          | 3/500 [00:09<26:14,  3.17s/it, loss: 4.76e-02]\rTraining:   1%|          | 4/500 [00:12<26:40,  3.23s/it, loss: 4.76e-02]\rTraining:   1%|          | 4/500 [00:12<26:40,  3.23s/it, loss: 2.17e-02]\rTraining:   1%|          | 5/500 [00:16<27:20,  3.31s/it, loss: 2.17e-02]\rTraining:   1%|          | 5/500 [00:16<27:20,  3.31s/it, loss: 2.30e-02]\rTraining:   1%|          | 6/500 [00:19<26:46,  3.25s/it, loss: 2.30e-02]\rTraining:   1%|          | 6/500 [00:19<26:46,  3.25s/it, loss: 2.27e-02]\rTraining:   1%|▏         | 7/500 [00:22<25:53,  3.15s/it, loss: 2.27e-02]\rTraining:   1%|▏         | 7/500 [00:22<25:53,  3.15s/it, loss: 2.10e-02]\rTraining:   2%|▏         | 8/500 [00:25<25:21,  3.09s/it, loss: 2.10e-02]\rTraining:   2%|▏         | 8/500 [00:25<25:21,  3.09s/it, loss: 1.99e-02]\rTraining:   2%|▏         | 9/500 [00:28<24:18,  2.97s/it, loss: 1.99e-02]\rTraining:   2%|▏         | 9/500 [00:28<24:18,  2.97s/it, loss: 1.87e-02]\rTraining:   2%|▏         | 10/500 [00:31<24:06,  2.95s/it, loss: 1.87e-02]\rTraining:   2%|▏         | 10/500 [00:31<24:06,  2.95s/it, loss: 1.74e-02]\rTraining:   2%|▏         | 11/500 [00:34<24:01,  2.95s/it, loss: 1.74e-02]\rTraining:   2%|▏         | 11/500 [00:34<24:01,  2.95s/it, loss: 1.68e-02]\rTraining:   2%|▏         | 12/500 [00:37<24:02,  2.96s/it, loss: 1.68e-02]\rTraining:   2%|▏         | 12/500 [00:37<24:02,  2.96s/it, loss: 1.55e-02]\rTraining:   3%|▎         | 13/500 [00:39<23:56,  2.95s/it, loss: 1.55e-02]\rTraining:   3%|▎         | 13/500 [00:39<23:56,  2.95s/it, loss: 1.49e-02]\rTraining:   3%|▎         | 14/500 [00:43<24:16,  3.00s/it, loss: 1.49e-02]\rTraining:   3%|▎         | 14/500 [00:43<24:16,  3.00s/it, loss: 1.20e-02]\rTraining:   3%|▎         | 15/500 [00:46<24:23,  3.02s/it, loss: 1.20e-02]\rTraining:   3%|▎         | 15/500 [00:46<24:23,  3.02s/it, loss: 1.18e-02]\rTraining:   3%|▎         | 16/500 [00:49<24:39,  3.06s/it, loss: 1.18e-02]\rTraining:   3%|▎         | 16/500 [00:49<24:39,  3.06s/it, loss: 1.19e-02]\rTraining:   3%|▎         | 17/500 [00:52<24:26,  3.04s/it, loss: 1.19e-02]\rTraining:   3%|▎         | 17/500 [00:52<24:26,  3.04s/it, loss: 1.22e-02]\rTraining:   4%|▎         | 18/500 [00:55<24:11,  3.01s/it, loss: 1.22e-02]\rTraining:   4%|▎         | 18/500 [00:55<24:11,  3.01s/it, loss: 1.11e-02]\rTraining:   4%|▍         | 19/500 [00:58<24:08,  3.01s/it, loss: 1.11e-02]\rTraining:   4%|▍         | 19/500 [00:58<24:08,  3.01s/it, loss: 1.04e-02]\rTraining:   4%|▍         | 20/500 [01:01<25:03,  3.13s/it, loss: 1.04e-02]\rTraining:   4%|▍         | 20/500 [01:01<25:03,  3.13s/it, loss: 9.50e-03]\rTraining:   4%|▍         | 21/500 [01:04<24:56,  3.13s/it, loss: 9.50e-03]\rTraining:   4%|▍         | 21/500 [01:04<24:56,  3.13s/it, loss: 8.76e-03]\rTraining:   4%|▍         | 22/500 [01:07<24:35,  3.09s/it, loss: 8.76e-03]\rTraining:   4%|▍         | 22/500 [01:07<24:35,  3.09s/it, loss: 8.21e-03]\rTraining:   5%|▍         | 23/500 [01:10<24:30,  3.08s/it, loss: 8.21e-03]\rTraining:   5%|▍         | 23/500 [01:10<24:30,  3.08s/it, loss: 7.75e-03]\rTraining:   5%|▍         | 24/500 [01:13<24:07,  3.04s/it, loss: 7.75e-03]\rTraining:   5%|▍         | 24/500 [01:13<24:07,  3.04s/it, loss: 7.77e-03]\rTraining:   5%|▌         | 25/500 [01:16<23:54,  3.02s/it, loss: 7.77e-03]\rTraining:   5%|▌         | 25/500 [01:16<23:54,  3.02s/it, loss: 7.39e-03]\rTraining:   5%|▌         | 26/500 [01:19<22:57,  2.91s/it, loss: 7.39e-03]\rTraining:   5%|▌         | 26/500 [01:19<22:57,  2.91s/it, loss: 7.38e-03]\rTraining:   5%|▌         | 27/500 [01:22<22:59,  2.92s/it, loss: 7.38e-03]\rTraining:   5%|▌         | 27/500 [01:22<22:59,  2.92s/it, loss: 7.10e-03]\rTraining:   6%|▌         | 28/500 [01:25<23:06,  2.94s/it, loss: 7.10e-03]\rTraining:   6%|▌         | 28/500 [01:25<23:06,  2.94s/it, loss: 7.06e-03]\rTraining:   6%|▌         | 29/500 [01:28<23:43,  3.02s/it, loss: 7.06e-03]\rTraining:   6%|▌         | 29/500 [01:28<23:43,  3.02s/it, loss: 6.74e-03]\rTraining:   6%|▌         | 30/500 [01:31<23:36,  3.01s/it, loss: 6.74e-03]\rTraining:   6%|▌         | 30/500 [01:31<23:36,  3.01s/it, loss: 7.16e-03]\rTraining:   6%|▌         | 31/500 [01:34<23:32,  3.01s/it, loss: 7.16e-03]\rTraining:   6%|▌         | 31/500 [01:34<23:32,  3.01s/it, loss: 6.83e-03]\rTraining:   6%|▋         | 32/500 [01:37<23:22,  3.00s/it, loss: 6.83e-03]\rTraining:   6%|▋         | 32/500 [01:37<23:22,  3.00s/it, loss: 6.97e-03]\rTraining:   7%|▋         | 33/500 [01:40<23:29,  3.02s/it, loss: 6.97e-03]\rTraining:   7%|▋         | 33/500 [01:40<23:29,  3.02s/it, loss: 6.71e-03]\rTraining:   7%|▋         | 34/500 [01:43<23:31,  3.03s/it, loss: 6.71e-03]\rTraining:   7%|▋         | 34/500 [01:43<23:31,  3.03s/it, loss: 7.53e-03]\rTraining:   7%|▋         | 35/500 [01:46<23:22,  3.02s/it, loss: 7.53e-03]\rTraining:   7%|▋         | 35/500 [01:46<23:22,  3.02s/it, loss: 7.12e-03]\rTraining:   7%|▋         | 36/500 [01:49<23:17,  3.01s/it, loss: 7.12e-03]\rTraining:   7%|▋         | 36/500 [01:49<23:17,  3.01s/it, loss: 6.19e-03]\rTraining:   7%|▋         | 37/500 [01:52<23:08,  3.00s/it, loss: 6.19e-03]\rTraining:   7%|▋         | 37/500 [01:52<23:08,  3.00s/it, loss: 5.64e-03]\rTraining:   8%|▊         | 38/500 [01:55<22:56,  2.98s/it, loss: 5.64e-03]\rTraining:   8%|▊         | 38/500 [01:55<22:56,  2.98s/it, loss: 5.31e-03]\rTraining:   8%|▊         | 39/500 [01:58<22:46,  2.96s/it, loss: 5.31e-03]\rTraining:   8%|▊         | 39/500 [01:58<22:46,  2.96s/it, loss: 5.50e-03]\rTraining:   8%|▊         | 40/500 [02:01<22:03,  2.88s/it, loss: 5.50e-03]\rTraining:   8%|▊         | 40/500 [02:01<22:03,  2.88s/it, loss: 5.39e-03]\rTraining:   8%|▊         | 41/500 [02:04<22:58,  3.00s/it, loss: 5.39e-03]\rTraining:   8%|▊         | 41/500 [02:04<22:58,  3.00s/it, loss: 5.11e-03]\rTraining:   8%|▊         | 42/500 [02:07<22:53,  3.00s/it, loss: 5.11e-03]\rTraining:   8%|▊         | 42/500 [02:07<22:53,  3.00s/it, loss: 5.14e-03]\rTraining:   9%|▊         | 43/500 [02:10<22:47,  2.99s/it, loss: 5.14e-03]\rTraining:   9%|▊         | 43/500 [02:10<22:47,  2.99s/it, loss: 5.78e-03]\rTraining:   9%|▉         | 44/500 [02:13<23:03,  3.03s/it, loss: 5.78e-03]\rTraining:   9%|▉         | 44/500 [02:13<23:03,  3.03s/it, loss: 4.65e-03]\rTraining:   9%|▉         | 45/500 [02:18<26:40,  3.52s/it, loss: 4.65e-03]\rTraining:   9%|▉         | 45/500 [02:18<26:40,  3.52s/it, loss: 4.47e-03]\rTraining:   9%|▉         | 46/500 [02:22<27:56,  3.69s/it, loss: 4.47e-03]\rTraining:   9%|▉         | 46/500 [02:22<27:56,  3.69s/it, loss: 4.98e-03]\rTraining:   9%|▉         | 47/500 [02:25<26:17,  3.48s/it, loss: 4.98e-03]\rTraining:   9%|▉         | 47/500 [02:25<26:17,  3.48s/it, loss: 4.32e-03]\rTraining:  10%|▉         | 48/500 [02:28<25:11,  3.34s/it, loss: 4.32e-03]\rTraining:  10%|▉         | 48/500 [02:28<25:11,  3.34s/it, loss: 4.47e-03]\rTraining:  10%|▉         | 49/500 [02:30<23:35,  3.14s/it, loss: 4.47e-03]\rTraining:  10%|▉         | 49/500 [02:30<23:35,  3.14s/it, loss: 4.05e-03]\rTraining:  10%|█         | 50/500 [02:33<23:09,  3.09s/it, loss: 4.05e-03]\rTraining:  10%|█         | 50/500 [02:33<23:09,  3.09s/it, loss: 4.18e-03]\rTraining:  10%|█         | 51/500 [02:36<22:54,  3.06s/it, loss: 4.18e-03]\rTraining:  10%|█         | 51/500 [02:36<22:54,  3.06s/it, loss: 4.13e-03]\rTraining:  10%|█         | 52/500 [02:39<22:43,  3.04s/it, loss: 4.13e-03]\rTraining:  10%|█         | 52/500 [02:39<22:43,  3.04s/it, loss: 4.28e-03]\rTraining:  11%|█         | 53/500 [02:42<22:27,  3.02s/it, loss: 4.28e-03]\rTraining:  11%|█         | 53/500 [02:42<22:27,  3.02s/it, loss: 4.00e-03]\rTraining:  11%|█         | 54/500 [02:46<24:19,  3.27s/it, loss: 4.00e-03]\rTraining:  11%|█         | 54/500 [02:46<24:19,  3.27s/it, loss: 4.37e-03]\rTraining:  11%|█         | 55/500 [02:49<23:33,  3.18s/it, loss: 4.37e-03]\rTraining:  11%|█         | 55/500 [02:49<23:33,  3.18s/it, loss: 4.05e-03]\rTraining:  11%|█         | 56/500 [02:52<23:05,  3.12s/it, loss: 4.05e-03]\rTraining:  11%|█         | 56/500 [02:52<23:05,  3.12s/it, loss: 4.10e-03]\rTraining:  11%|█▏        | 57/500 [02:56<23:34,  3.19s/it, loss: 4.10e-03]\rTraining:  11%|█▏        | 57/500 [02:56<23:34,  3.19s/it, loss: 3.69e-03]\rTraining:  12%|█▏        | 58/500 [03:00<26:30,  3.60s/it, loss: 3.69e-03]\rTraining:  12%|█▏        | 58/500 [03:00<26:30,  3.60s/it, loss: 4.22e-03]\rTraining:  12%|█▏        | 59/500 [03:05<30:09,  4.10s/it, loss: 4.22e-03]\rTraining:  12%|█▏        | 59/500 [03:05<30:09,  4.10s/it, loss: 3.78e-03]\rTraining:  12%|█▏        | 60/500 [03:20<52:38,  7.18s/it, loss: 3.78e-03]\rTraining:  12%|█▏        | 60/500 [03:20<52:38,  7.18s/it, loss: 3.94e-03]\rTraining:  12%|█▏        | 61/500 [03:23<43:18,  5.92s/it, loss: 3.94e-03]\rTraining:  12%|█▏        | 61/500 [03:23<43:18,  5.92s/it, loss: 3.93e-03]\rTraining:  12%|█▏        | 62/500 [03:26<37:32,  5.14s/it, loss: 3.93e-03]\rTraining:  12%|█▏        | 62/500 [03:26<37:32,  5.14s/it, loss: 3.65e-03]\rTraining:  13%|█▎        | 63/500 [03:29<32:45,  4.50s/it, loss: 3.65e-03]\rTraining:  13%|█▎        | 63/500 [03:29<32:45,  4.50s/it, loss: 3.87e-03]\rTraining:  13%|█▎        | 64/500 [03:32<29:22,  4.04s/it, loss: 3.87e-03]\rTraining:  13%|█▎        | 64/500 [03:32<29:22,  4.04s/it, loss: 3.83e-03]\rTraining:  13%|█▎        | 65/500 [03:35<26:13,  3.62s/it, loss: 3.83e-03]\rTraining:  13%|█▎        | 65/500 [03:35<26:13,  3.62s/it, loss: 3.92e-03]\rTraining:  13%|█▎        | 66/500 [03:38<26:46,  3.70s/it, loss: 3.92e-03]\rTraining:  13%|█▎        | 66/500 [03:38<26:46,  3.70s/it, loss: 3.62e-03]\rTraining:  13%|█▎        | 67/500 [03:42<26:21,  3.65s/it, loss: 3.62e-03]\rTraining:  13%|█▎        | 67/500 [03:42<26:21,  3.65s/it, loss: 4.03e-03]\rTraining:  14%|█▎        | 68/500 [03:45<24:52,  3.45s/it, loss: 4.03e-03]\rTraining:  14%|█▎        | 68/500 [03:45<24:52,  3.45s/it, loss: 3.55e-03]"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:717)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:458)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:716)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:508)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:509)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:68)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:694)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:874)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$7(Chauffeur.scala:900)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:899)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:954)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:742)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:508)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:509)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1032)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:952)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:552)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:521)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:766)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:46)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:766)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:729)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:711)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$16(ActivityContextFactory.scala:283)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:46)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:283)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:521)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:411)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:126)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:123)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:717)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:458)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:716)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:508)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:509)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:68)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:694)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:874)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$7(Chauffeur.scala:900)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:899)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:954)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:742)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:508)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:509)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1032)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:952)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:552)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:521)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:766)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:46)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:766)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:729)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:711)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$16(ActivityContextFactory.scala:283)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:46)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:283)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:521)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:411)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:126)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:123)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = torch.jit.script(DiffEqLayer(TNNCell).to(device))\n",
    "loss_func = nn.MSELoss(reduction=\"none\")\n",
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "n_epochs = 500 #100\n",
    "tbptt_size = 100 #512\n",
    "\n",
    "n_batches = np.ceil(train_tensor.shape[0] / tbptt_size).astype(int)\n",
    "with tqdm(desc=\"Training\", total=n_epochs) as pbar:\n",
    "    for epoch in range(n_epochs):\n",
    "        # first state is ground truth temperature data\n",
    "        hidden = train_tensor[0, :, -len(target_cols) :]\n",
    "\n",
    "        # propagate batch-wise through data set\n",
    "        for i in range(n_batches):\n",
    "            model.zero_grad()\n",
    "            output, hidden = model(\n",
    "                train_tensor[\n",
    "                    i * tbptt_size : (i + 1) * tbptt_size, :, : len(input_cols)\n",
    "                ],\n",
    "                hidden.detach(),\n",
    "            )\n",
    "            loss = loss_func(\n",
    "                output,\n",
    "                train_tensor[\n",
    "                    i * tbptt_size : (i + 1) * tbptt_size, :, -len(target_cols) :\n",
    "                ],\n",
    "            )\n",
    "            # sample_weighting\n",
    "            loss = (\n",
    "                (\n",
    "                    loss\n",
    "                    * train_sample_weights[\n",
    "                        i * tbptt_size : (i + 1) * tbptt_size, :, None\n",
    "                    ]\n",
    "                    / train_sample_weights[\n",
    "                        i * tbptt_size : (i + 1) * tbptt_size, :\n",
    "                    ].sum()\n",
    "                )\n",
    "                .sum()\n",
    "                .mean()\n",
    "            )\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # reduce learning rate\n",
    "        if epoch == int(n_epochs*0.75):\n",
    "            for group in opt.param_groups:\n",
    "                group[\"lr\"] *= 0.5\n",
    "        pbar.update()\n",
    "        pbar.set_postfix_str(f\"loss: {loss.item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb76976b-f810-4646-b2af-693889fc3a9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bbd6ebe-4b95-49e0-9cb3-aebc96804c39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# model saving and loading\n",
    "mdl_path = Path.cwd() / 'data' / 'models'\n",
    "mdl_path.mkdir(exist_ok=True, parents=True)\n",
    "# mdl_file_path = mdl_path / 'tnn_jit_torch.pt'\n",
    "mdl_file_path = mdl_path / 'tnn_jit_torch_STM_RTM_Oil_Joint_EFAD.pt'\n",
    "# mdl_file_path = mdl_path / 'tnn_jit_torch_RTM_EFAD.pt'\n",
    "model.save(mdl_file_path)  # save\n",
    "model = torch.jit.load(mdl_file_path)  # load\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41c2b4ca-8c0c-4cbc-b918-6ffc57ba1c51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# evaluate against test set\n",
    "with torch.no_grad():\n",
    "    pred, hidden = model(\n",
    "        test_tensor[:, :, : len(input_cols)], test_tensor[0, :, -len(target_cols) :]\n",
    "    )\n",
    "    pred = pred.cpu().numpy() * 200  # denormalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e238672b-29f6-4a89-8b5c-3ebb7aca7401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualize Performance\n",
    "We see the performance for one trial, i.e., one run of random initialized weights.\n",
    "\n",
    "Usually, running multiple trials with different random number generator seeds yields even better results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b874819-d325-48fb-b033-5927f687f1a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(len(test_profiles), len(target_cols), figsize=(20, 10))\n",
    "\n",
    "\n",
    "# 保证 axes 是二维数组\n",
    "if len(test_profiles) == 1 and len(target_cols) == 1:\n",
    "    axes = np.array([[axes]])\n",
    "elif len(test_profiles) == 1:\n",
    "    axes = axes[np.newaxis, :]\n",
    "elif len(target_cols) == 1:\n",
    "    axes = axes[:, np.newaxis]\n",
    "\n",
    "\n",
    "for i, (pid, y_test) in enumerate(\n",
    "    data.loc[data.profile_id.isin(test_profiles), target_cols + [\"profile_id\"]].groupby(\n",
    "        \"profile_id\"\n",
    "    )\n",
    "):\n",
    "    y_test *= 200\n",
    "    profile_pred = pred[: len(y_test), i, :]\n",
    "    for j, col in enumerate(target_cols):\n",
    "        ax = axes[i, j]\n",
    "        ax.plot(\n",
    "            y_test.loc[:, col].reset_index(drop=True),\n",
    "            color=\"tab:green\",\n",
    "            label=\"Ground truth\",\n",
    "        )\n",
    "        ax.plot(profile_pred[:, j], color=\"tab:blue\", label=\"Prediction\")\n",
    "        ax.text(\n",
    "            x=0.5,\n",
    "            y=0.8,\n",
    "            s=f\"MSE: {((profile_pred[:, j] - y_test.loc[:, col])**2).sum() / len(profile_pred):.3f} K²\\nmax.abs.:{(profile_pred[:, j]-y_test.loc[:, col]).abs().max():.1f} K\",\n",
    "            transform=ax.transAxes,\n",
    "        )\n",
    "        if j == 0:\n",
    "            ax.set_ylabel(f\"Profile {pid}\\n Temp. in °C\")\n",
    "            if i == 0:\n",
    "                ax.legend()\n",
    "        if i == len(test_profiles) - 1:\n",
    "            ax.set_xlabel(f\"Iters\")\n",
    "        elif i == 0:\n",
    "            ax.set_title(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a129d597-7da0-4e8f-999f-3af40c96f25e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The performance that is achievable by the hybridization of LPTNs with neural networks is unprecedented and not achievable by pure LPTN or pure black-box ML models.\n",
    "Note that the visualized performance stems from training a TNN from scratch once. All neural networks are initialized randomly when their training by gradient descent begins.\n",
    "This means that better performance can be easily achieved by repeating this experiment since the convergence into better local minima becomes likely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9af79340-1af5-4855-b6b8-f9f340df021e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# evaluate against test set\n",
    "with torch.no_grad():\n",
    "    pred_train, hidden_train = model(\n",
    "        train_tensor[:, :, : len(input_cols)], train_tensor[0, :, -len(target_cols) :]\n",
    "    )\n",
    "    pred_train = pred_train.cpu().numpy() * 200  # denormalize\n",
    "\n",
    "data_train = data.copy()\n",
    "data_train.loc[data_train.profile_id.isin(train_profiles), target_cols] *=200 # denormalize\n",
    "results_cmbd_merged = pd.DataFrame()\n",
    "target_estd_cols = [col + '_Estd' for col in target_cols]\n",
    "for i, (pid, y_train) in enumerate(\n",
    "    data_train.loc[data_train.profile_id.isin(train_profiles), target_cols + input_cols + [\"profile_id\"]].groupby(\n",
    "        \"profile_id\"\n",
    "    )\n",
    "):\n",
    "    results_ref = y_train.reset_index(drop=True).copy()\n",
    "    profile_pred = pred_train[: len(y_train), i, :]\n",
    "    results_estd = pd.DataFrame(profile_pred, columns=target_estd_cols) \n",
    "    results_cmbd = pd.concat([results_ref, results_estd], axis=1)\n",
    "    results_cmbd_merged = pd.concat([results_cmbd_merged, results_cmbd], ignore_index=True)\n",
    "    \n",
    "\n",
    "# Create subplots with 5 rows and 1 column, with third row having dual Y-axis\n",
    "fig = make_subplots(\n",
    "    rows=5, \n",
    "    cols=1, \n",
    "    shared_xaxes=True, \n",
    "    vertical_spacing=0.08,\n",
    "    specs=[\n",
    "        [{}],         # Row 1: Single Y-axis\n",
    "        [{}],         # Row 2: Single Y-axis\n",
    "        [{\"secondary_y\": True}],  # Row 3: Dual Y-axis\n",
    "        [{\"secondary_y\": True}],  # Row 4: Dual Y-axis\n",
    "        [{}]          # Row 5: Single Y-axis\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Row 1: Actual and estimated values\n",
    "for col in target_cols:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=results_cmbd_merged.index,\n",
    "        y=results_cmbd_merged[col],\n",
    "        mode='lines',\n",
    "        name=col\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "for col in target_estd_cols:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=results_cmbd_merged.index,\n",
    "        y=results_cmbd_merged[col],\n",
    "        mode='lines',\n",
    "        name=col\n",
    "    ), row=1, col=1)\n",
    "\n",
    "# Row 2: Difference between estimated and actual values\n",
    "for col_actual, col_estd in zip(target_cols, target_estd_cols):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=results_cmbd_merged.index,\n",
    "        y=results_cmbd_merged[col_estd] - results_cmbd_merged[col_actual],\n",
    "        mode='lines',\n",
    "        name=f\"{col_estd} - {col_actual}\"\n",
    "    ), row=2, col=1)\n",
    "\n",
    "# Row 3: tqEm and nEm with dual Y-axis\n",
    "# tqEm - Left axis\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=results_cmbd_merged.index,\n",
    "    y=results_cmbd_merged['tqEmFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm'] * MeasurementData_Merged['tqEmFild100ms.Rec_10ms_Fild.pp_rbe_Mct_10ms_Fild.rbe_MctAsm'].abs().max(),\n",
    "    mode='lines',\n",
    "    name='tqEm'\n",
    "), row=3, col=1, secondary_y=False)\n",
    "\n",
    "# nEm - Right axis\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=results_cmbd_merged.index,\n",
    "    y=results_cmbd_merged['nEmFild100ms.Rec_10ms_Fild.pp_rbe_CddAgEm_10ms_Fild.rbe_CddRslvr'] * MeasurementData_Merged['nEmFild100ms.Rec_10ms_Fild.pp_rbe_CddAgEm_10ms_Fild.rbe_CddRslvr'].abs().max(),\n",
    "    mode='lines',\n",
    "    name='nEm'\n",
    "), row=3, col=1, secondary_y=True)\n",
    "\n",
    "# Row 4: tCoolt and VfCoolt with dual Y-axis\n",
    "# tqEm - Left axis\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=results_cmbd_merged.index,\n",
    "    y=results_cmbd_merged['R_CW_tCooltIvtrOut'] * MeasurementData_Merged['R_CW_tCooltIvtrOut'].abs().max(),\n",
    "    mode='lines',\n",
    "    name='tCooltInvtrOut'\n",
    "), row=4, col=1, secondary_y=False)\n",
    "\n",
    "# # VfCoolt - Right axis\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=results_cmbd_merged.index,\n",
    "#     y=,\n",
    "#     mode='lines',\n",
    "#     name='VfCoolt'\n",
    "# ), row=4, col=1, secondary_y=True)\n",
    "\n",
    "# Row 5: Profile ID\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=results_cmbd_merged.index,\n",
    "    y=results_cmbd_merged['profile_id'],\n",
    "    mode='lines',\n",
    "    name=\"Profile ID\"\n",
    "), row=5, col=1)\n",
    "\n",
    "# Layout configuration\n",
    "fig.update_layout(\n",
    "    height=1200,\n",
    "    # width=1200,\n",
    "    title_text=\"Actual vs Estimated Values Comparison\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "# Configure axis titles\n",
    "fig.update_yaxes(title_text=\"Act/Estd Temp (°C)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Est. Err. (°C)\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"tqEm (Nm)\", row=3, col=1)\n",
    "fig.update_yaxes(\n",
    "    title_text=\"nEm (rpm)\", \n",
    "    overlaying=\"y3\",         # Overlay on row 3's primary Y-axis\n",
    "    side=\"right\",            # Position on right side\n",
    "    row=3, col=1,\n",
    "    secondary_y=True\n",
    ")\n",
    "fig.update_yaxes(title_text=\"tCooltIvtrOut (°C)\", row=4, col=1)\n",
    "fig.update_yaxes(title_text=\"Profile ID\", row=5, col=1)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "# 假设你已经创建了一个 Plotly 图表对象 fig\n",
    "pio.write_html(fig, file=f'{target_cols}.html', auto_open=False)\n",
    "\n",
    "# Plot Histogram figures\n",
    "for col_actual, col_estd in zip(target_cols, target_estd_cols):\n",
    "    # Calculate error distribution\n",
    "    error = results_cmbd_merged[col_estd] - results_cmbd_merged[col_actual]\n",
    "    mu, std = norm.fit(error)\n",
    "    xmin, xmax = error.min(), error.max()\n",
    "    \n",
    "    # Create histogram\n",
    "    hist = go.Histogram(\n",
    "        x=error,\n",
    "        histnorm='probability density',  # Normalize to PDF\n",
    "        nbinsx=100,                      # Match original bin count\n",
    "        marker=dict(color='green', opacity=0.6, line=dict(width=1, color='black')),\n",
    "        name='Error Distribution',\n",
    "        hoverinfo='x+y',\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Create fitted normal curve\n",
    "    x_fit = np.linspace(xmin, xmax, 100)\n",
    "    y_fit = norm.pdf(x_fit, mu, std)\n",
    "    fit_curve = go.Scatter(\n",
    "        x=x_fit,\n",
    "        y=y_fit,\n",
    "        mode='lines',\n",
    "        line=dict(color='black', width=2),\n",
    "        name='Normal Fit',\n",
    "        hoverinfo='none'\n",
    "    )\n",
    "    \n",
    "    # Create statistical reference lines\n",
    "    lines = [\n",
    "        # Mean line (red dashed)\n",
    "        go.Scatter(\n",
    "            x=[mu, mu],\n",
    "            y=[0, norm.pdf(mu, mu, std)*1.1],  # Height = 110% of peak density\n",
    "            mode='lines',\n",
    "            line=dict(color='red', width=2, dash='dash'),\n",
    "            name=f'μ = {mu:.2f}',\n",
    "            hoverinfo='x+name'\n",
    "        ),\n",
    "        # +3σ line (orange dotted)\n",
    "        go.Scatter(\n",
    "            x=[mu+3*std, mu+3*std],\n",
    "            y=[0, norm.pdf(mu, mu, std)*1.1],\n",
    "            mode='lines',\n",
    "            line=dict(color='orange', width=2, dash='dot'),\n",
    "            name=f'μ+3σ = {mu+3*std:.2f}',\n",
    "            hoverinfo='x+name'\n",
    "        ),\n",
    "        # -3σ line (orange dotted)\n",
    "        go.Scatter(\n",
    "            x=[mu-3*std, mu-3*std],\n",
    "            y=[0, norm.pdf(mu, mu, std)*1.1],\n",
    "            mode='lines',\n",
    "            line=dict(color='orange', width=2, dash='dot'),\n",
    "            name=f'μ-3σ = {mu-3*std:.2f}',\n",
    "            hoverinfo='x+name'\n",
    "        ),\n",
    "        # Min error line (blue dotted)\n",
    "        go.Scatter(\n",
    "            x=[xmin, xmin],\n",
    "            y=[0, norm.pdf(mu, mu, std)*1.1],\n",
    "            mode='lines',\n",
    "            line=dict(color='blue', width=2, dash='dot'),\n",
    "            name=f'Min Error = {xmin:.2f}',\n",
    "            hoverinfo='x+name'\n",
    "        ),\n",
    "        # Max error line (blue dotted)\n",
    "        go.Scatter(\n",
    "            x=[xmax, xmax],\n",
    "            y=[0, norm.pdf(mu, mu, std)*1.1],\n",
    "            mode='lines',\n",
    "            line=dict(color='blue', width=2, dash='dot'),\n",
    "            name=f'Max Error = {xmax:.2f}',\n",
    "            hoverinfo='x+name'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Combine all traces\n",
    "    data2plot = [hist, fit_curve] + lines\n",
    "    \n",
    "    # Create layout with interactive features\n",
    "    layout = go.Layout(\n",
    "        title=f\"Fitting results {col_actual}: μ = {mu:.2f}, σ = {std:.2f}\",\n",
    "        xaxis=dict(title='Estd-Measd (K)', showgrid=True),\n",
    "        yaxis=dict(title='Probability Density', showgrid=True),\n",
    "        hovermode='closest',\n",
    "        legend=dict(\n",
    "            orientation='h',\n",
    "            yanchor='bottom',\n",
    "            y=1.02,\n",
    "            xanchor='right',\n",
    "            x=1\n",
    "        ),\n",
    "        height=500,\n",
    "        width=800,\n",
    "        margin=dict(t=150, b=80),\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    \n",
    "    # Create and show figure\n",
    "    fig = go.Figure(data=data2plot, layout=layout)\n",
    "    fig.show()\n",
    "    # 假设你已经创建了一个 Plotly 图表对象 fig\n",
    "    pio.write_html(fig, file=f'{col_actual}_Histogram.html', auto_open=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5db320f-236d-49e8-ba47-72ab423bab02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Back-up Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27fa2b54-c361-4746-b7a1-ba3a341f317b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Hyper Parameters Optimization (optuna method based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ef1fcb-39fe-46a4-b6c6-c1e6cca8f183",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-16 17:18:18,379] A new study created in memory with name: no-name-dcbacc97-0406-42de-ba9f-baea6b10b6e1\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c72442726c4eb09fba7d151dc994e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.CommandCancelledException\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)\n",
       "\tat scala.collection.immutable.Range.foreach(Range.scala:158)\n",
       "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:717)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)\n",
       "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:458)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:716)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:508)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:68)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:509)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:68)\n",
       "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:694)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:874)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$7(Chauffeur.scala:900)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:899)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:954)\n",
       "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:742)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:508)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:509)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1032)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:952)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:552)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:521)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:766)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:46)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:766)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:729)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:711)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$16(ActivityContextFactory.scala:283)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:46)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:283)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:521)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:411)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)\n",
       "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:126)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:123)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.base/java.lang.Thread.run(Thread.java:840)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Cancelled"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.CommandCancelledException",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$5(SequenceExecutionState.scala:136)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3(SequenceExecutionState.scala:136)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.$anonfun$cancel$3$adapted(SequenceExecutionState.scala:133)",
        "\tat scala.collection.immutable.Range.foreach(Range.scala:158)",
        "\tat com.databricks.spark.chauffeur.SequenceExecutionState.cancel(SequenceExecutionState.scala:133)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancelRunningSequence(ExecContextState.scala:717)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.$anonfun$cancel$1(ExecContextState.scala:435)",
        "\tat scala.Option.getOrElse(Option.scala:189)",
        "\tat com.databricks.spark.chauffeur.ExecContextState.cancel(ExecContextState.scala:435)",
        "\tat com.databricks.spark.chauffeur.ExecutionContextManagerV1.cancelExecution(ExecutionContextManagerV1.scala:458)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.$anonfun$process$1(ChauffeurState.scala:716)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:508)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionContext(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.withAttributionTags(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperationWithResultTags(ChauffeurState.scala:68)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:509)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.recordOperation(ChauffeurState.scala:68)",
        "\tat com.databricks.spark.chauffeur.ChauffeurState.process(ChauffeurState.scala:694)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequest$1(Chauffeur.scala:874)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.$anonfun$applyOrElse$7(Chauffeur.scala:900)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.handleDriverRequestWithUsageLogging$1(Chauffeur.scala:899)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:954)",
        "\tat com.databricks.spark.chauffeur.Chauffeur$$anon$1$$anonfun$receive$1.applyOrElse(Chauffeur.scala:742)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:174)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:200)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:171)",
        "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$1(ServerBackend.scala:147)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:508)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:613)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:636)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:22)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:96)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:77)",
        "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:608)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:517)",
        "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:22)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:509)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)",
        "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:22)",
        "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:146)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:1032)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:952)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6(JettyServer.scala:552)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$6$adapted(JettyServer.scala:521)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:766)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:46)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:766)",
        "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:729)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:711)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$16(ActivityContextFactory.scala:283)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:46)",
        "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:283)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:521)",
        "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:411)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)",
        "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)",
        "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)",
        "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)",
        "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)",
        "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)",
        "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)",
        "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)",
        "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)",
        "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)",
        "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)",
        "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)",
        "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$DecryptedEndPoint.onFillable(SslConnection.java:555)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:410)",
        "\tat org.eclipse.jetty.io.ssl.SslConnection$2.succeeded(SslConnection.java:164)",
        "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)",
        "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)",
        "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)",
        "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:295)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:291)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:110)",
        "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:126)",
        "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:123)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:45)",
        "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:92)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)",
        "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)",
        "\tat java.base/java.lang.Thread.run(Thread.java:840)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 自定义正弦激活层\n",
    "class SinusLayer(nn.Module):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sin(x)\n",
    "\n",
    "# 激活函数映射\n",
    "def get_activation(activation_name: str) -> nn.Module:\n",
    "    activation_dict = {\n",
    "        \"Sigmoid\": nn.Sigmoid(),\n",
    "        \"tanh\": nn.Tanh(),\n",
    "        \"linear\": nn.Identity(),\n",
    "        \"ReLU\": nn.ReLU(),\n",
    "        \"biased Elu\": nn.ELU(alpha=1.0),\n",
    "        \"sinus\": SinusLayer()\n",
    "    }\n",
    "    return activation_dict[activation_name]\n",
    "\n",
    "# TNNCell定义（支持动态结构和激活函数）\n",
    "class TNNCell(nn.Module):\n",
    "    def __init__(self, cond_net_layers, cond_net_units, cond_activation, \n",
    "                 ploss_net_layers, ploss_net_units, ploss_activation):\n",
    "        super().__init__()\n",
    "        self.sample_time = 0.5\n",
    "        self.output_size = len(target_cols)\n",
    "        self.caps = nn.Parameter(torch.Tensor(self.output_size))\n",
    "        nn.init.normal_(self.caps, mean=-9.2, std=0.5)\n",
    "        \n",
    "        n_temps = len(temperature_cols)\n",
    "        n_conds = int(0.5 * n_temps * (n_temps - 1))\n",
    "        \n",
    "        # 动态构建conductance_net\n",
    "        cond_layers = []\n",
    "        input_dim = len(input_cols) + self.output_size\n",
    "        for units in cond_net_units:\n",
    "            cond_layers.append(nn.Linear(input_dim, units))\n",
    "            cond_layers.append(get_activation(cond_activation))\n",
    "            input_dim = units\n",
    "        cond_layers.append(nn.Linear(input_dim, n_conds))\n",
    "        cond_layers.append(nn.Sigmoid())\n",
    "        self.conductance_net = nn.Sequential(*cond_layers)\n",
    "        \n",
    "        # 动态构建ploss_net\n",
    "        ploss_layers = []\n",
    "        input_dim = len(input_cols) + self.output_size\n",
    "        for i, units in enumerate(ploss_net_units):\n",
    "            ploss_layers.append(nn.Linear(input_dim, units))\n",
    "            if i < len(ploss_net_units) - 1:\n",
    "                ploss_layers.append(get_activation(ploss_activation))\n",
    "            input_dim = units\n",
    "        ploss_layers.append(nn.Linear(input_dim, self.output_size))\n",
    "        self.ploss = nn.Sequential(*ploss_layers)\n",
    "        \n",
    "        # 其余初始化代码\n",
    "        self.adj_mat = np.zeros((n_temps, n_temps), dtype=int)\n",
    "        triu_idx = np.triu_indices(n_temps, 1)\n",
    "        adj_idx_arr = np.ones_like(self.adj_mat)\n",
    "        adj_idx_arr = adj_idx_arr[triu_idx].ravel()\n",
    "        self.adj_mat[triu_idx] = np.cumsum(adj_idx_arr) - 1\n",
    "        self.adj_mat += self.adj_mat.T\n",
    "        self.adj_mat = torch.from_numpy(self.adj_mat[:self.output_size, :]).type(torch.int64)\n",
    "        self.temp_idcs = [i for i, x in enumerate(input_cols) if x in temperature_cols]\n",
    "        self.nontemp_idcs = [i for i, x in enumerate(input_cols) if x not in temperature_cols + [\"profile_id\"]]\n",
    "\n",
    "    def forward(self, inp: Tensor, hidden: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        prev_out = hidden\n",
    "        temps = torch.cat([prev_out, inp[:, self.temp_idcs]], dim=1)\n",
    "        sub_nn_inp = torch.cat([inp, prev_out], dim=1)\n",
    "        conducts = torch.abs(self.conductance_net(sub_nn_inp))\n",
    "        power_loss = torch.abs(self.ploss(sub_nn_inp))\n",
    "        temp_diffs = torch.sum(\n",
    "            (temps.unsqueeze(1) - prev_out.unsqueeze(-1)) * conducts[:, self.adj_mat],\n",
    "            dim=-1,\n",
    "        )\n",
    "        out = prev_out + self.sample_time * torch.exp(self.caps) * (temp_diffs + power_loss)\n",
    "        return prev_out, torch.clip(out, -1, 5)\n",
    "\n",
    "# DiffEqLayer定义（支持TorchScript）\n",
    "class DiffEqLayer(ScriptModule):\n",
    "    def __init__(self, cell_module):\n",
    "        super().__init__()\n",
    "        self.cell = cell_module\n",
    "        \n",
    "    @script_method\n",
    "    def forward(self, input: Tensor, state: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        inputs = input.unbind(0)\n",
    "        outputs = []\n",
    "        for i in range(len(inputs)):\n",
    "            out, state = self.cell(inputs[i], state)\n",
    "            outputs.append(out)\n",
    "        return torch.stack(outputs), state\n",
    "\n",
    "# Optuna目标函数\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    # 1. 超参数采样\n",
    "    cond_net_layers = trial.suggest_int('cond_net_layers', 1, 3)\n",
    "    cond_net_units = [trial.suggest_int(f'cond_net_units_{i}', 2, 30) \n",
    "                     for i in range(cond_net_layers)]\n",
    "    \n",
    "    ploss_net_layers = trial.suggest_int('ploss_net_layers', 1, 3)\n",
    "    ploss_net_units = [trial.suggest_int(f'ploss_net_units_{i}', 2, 30) \n",
    "                      for i in range(ploss_net_layers)]\n",
    "    \n",
    "    cond_activation = trial.suggest_categorical(\n",
    "        \"cond_activation\", \n",
    "        [\"Sigmoid\", \"tanh\", \"linear\", \"ReLU\", \"biased Elu\", \"sinus\"]\n",
    "    )\n",
    "    ploss_activation = trial.suggest_categorical(\n",
    "        \"ploss_activation\", \n",
    "        [\"Sigmoid\", \"tanh\", \"linear\", \"ReLU\", \"biased Elu\", \"sinus\"]\n",
    "    )\n",
    "    \n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'NAdam'])\n",
    "    tbptt_size = trial.suggest_int('tbptt_size', 50, 512)\n",
    "\n",
    "    # 2. 构建模型（支持TorchScript）\n",
    "    tnn_cell = TNNCell(\n",
    "        cond_net_layers, cond_net_units, cond_activation,\n",
    "        ploss_net_layers, ploss_net_units, ploss_activation\n",
    "    ).to(device)\n",
    "    \n",
    "    model = DiffEqLayer(tnn_cell).to(device)\n",
    "    \n",
    "    # 3. 选择优化器\n",
    "    if optimizer_name == 'Adam':\n",
    "        opt = optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        opt = optim.NAdam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # 4. 训练与评估（使用训练集本身）\n",
    "    n_epochs = 100\n",
    "    loss_func = nn.MSELoss(reduction=\"none\")\n",
    "    best_train_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        hidden = train_tensor[0, :, -len(target_cols):].detach()\n",
    "        n_batches = int(np.ceil(train_tensor.shape[0] / tbptt_size))\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        model.train()\n",
    "        for i in range(n_batches):\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            output, hidden = model(\n",
    "                train_tensor[i*tbptt_size : (i+1)*tbptt_size, :, :len(input_cols)],\n",
    "                hidden.detach()\n",
    "            )\n",
    "            \n",
    "            # 计算训练损失\n",
    "            loss = loss_func(\n",
    "                output,\n",
    "                train_tensor[i*tbptt_size : (i+1)*tbptt_size, :, -len(target_cols):]\n",
    "            )\n",
    "            \n",
    "            # 加权损失计算\n",
    "            loss = (loss * train_sample_weights[i*tbptt_size:(i+1)*tbptt_size, :, None]).sum() \n",
    "            loss /= train_sample_weights[i*tbptt_size:(i+1)*tbptt_size, :].sum() + 1e-8  # 防止除零\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # 计算平均训练损失\n",
    "        avg_epoch_loss = epoch_loss / n_batches\n",
    "        \n",
    "        # 报告训练损失给Optuna\n",
    "        trial.report(avg_epoch_loss, epoch)\n",
    "        \n",
    "        # 剪枝逻辑（基于训练损失）\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        # 记录最佳训练损失\n",
    "        if avg_epoch_loss < best_train_loss:\n",
    "            best_train_loss = avg_epoch_loss\n",
    "    \n",
    "    return best_train_loss\n",
    "\n",
    "# 创建Optuna研究\n",
    "study = optuna.create_study(\n",
    "    direction='minimize',\n",
    "    sampler=optuna.samplers.TPESampler(),\n",
    "    pruner=optuna.pruners.MedianPruner(\n",
    "        n_startup_trials=5,\n",
    "        n_warmup_steps=10\n",
    "    )\n",
    ")\n",
    "\n",
    "# 启动优化\n",
    "study.optimize(objective, n_trials=200, show_progress_bar=True)\n",
    "\n",
    "# 输出结果\n",
    "print(\"最佳训练损失:\", study.best_value)\n",
    "print(\"最佳参数组合:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# 使用最佳参数训练最终模型\n",
    "best_params = study.best_params\n",
    "final_tnn_cell = TNNCell(\n",
    "    cond_net_layers=best_params['cond_net_layers'],\n",
    "    cond_net_units=[best_params[f'cond_net_units_{i}'] for i in range(best_params['cond_net_layers'])],\n",
    "    cond_activation=best_params['cond_activation'],\n",
    "    ploss_net_layers=best_params['ploss_net_layers'],\n",
    "    ploss_net_units=[best_params[f'ploss_net_units_{i}'] for i in range(best_params['ploss_net_layers'])],\n",
    "    ploss_activation=best_params['ploss_activation']\n",
    ").to(device)\n",
    "\n",
    "final_model = torch.jit.script(DiffEqLayer(final_tnn_cell)).to(device)\n",
    "# 从Optuna最佳参数中提取优化器名称\n",
    "optimizer_name = best_params['optimizer']\n",
    "# 动态创建优化器\n",
    "if optimizer_name == 'Adam':\n",
    "    opt = optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
    "else:  # 对应NAdam\n",
    "    opt = optim.NAdam(final_model.parameters(), lr=best_params['lr'])\n",
    "# 最终训练\n",
    "n_epochs = 500\n",
    "tbptt_size = best_params['tbptt_size']\n",
    "\n",
    "with tqdm(desc=\"Final Training\", total=n_epochs) as pbar:\n",
    "    for epoch in range(n_epochs):\n",
    "        hidden = train_tensor[0, :, -len(target_cols):].detach()\n",
    "        n_batches = int(np.ceil(train_tensor.shape[0] / tbptt_size))\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            opt.zero_grad()\n",
    "            output, hidden = final_model(\n",
    "                train_tensor[i*tbptt_size : (i+1)*tbptt_size, :, :len(input_cols)],\n",
    "                hidden.detach()\n",
    "            )\n",
    "            \n",
    "            loss = loss_func(\n",
    "                output,\n",
    "                train_tensor[i*tbptt_size : (i+1)*tbptt_size, :, -len(target_cols):]\n",
    "            )\n",
    "            \n",
    "            loss = (loss * train_sample_weights[i*tbptt_size:(i+1)*tbptt_size, :, None]).sum()\n",
    "            loss /= train_sample_weights[i*tbptt_size:(i+1)*tbptt_size, :].sum()\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "        \n",
    "        # 学习率衰减\n",
    "        if epoch == int(n_epochs*0.75):\n",
    "            for group in opt.param_groups:\n",
    "                group[\"lr\"] *= 0.5\n",
    "        \n",
    "        pbar.update()\n",
    "        pbar.set_postfix_str(f\"loss: {loss.item():.2e}\")\n",
    "\n",
    "# model saving and loading\n",
    "mdl_path = Path.cwd() / 'data' / 'models'\n",
    "mdl_path.mkdir(exist_ok=True, parents=True)\n",
    "# mdl_file_path = mdl_path / 'tnn_jit_torch.pt'\n",
    "mdl_file_path = mdl_path / 'tnn_jit_torch_STM_RTM_Oil_Joint_EFAD.pt'\n",
    "# mdl_file_path = mdl_path / 'tnn_jit_torch_RTM_EFAD.pt'\n",
    "final_model.save(mdl_file_path)  # save\n",
    "final_model = torch.jit.load(mdl_file_path)  # load\n",
    "final_model.eval()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-06-16 18:28:21",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}